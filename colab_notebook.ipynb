{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and install required packages (if not already available)\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def check_and_install_package(package_name, import_name=None):\n",
    "    \"\"\"Check if package is installed, install if not\"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package_name\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f\"âœ“ {package_name} is already installed\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"âš  {package_name} not found, installing...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--user\", package_name])\n",
    "        return False\n",
    "\n",
    "# Check for required packages\n",
    "required_packages = [\n",
    "    (\"torch\", \"torch\"),\n",
    "    (\"torchvision\", \"torchvision\"),\n",
    "    (\"timm\", \"timm\"),\n",
    "    (\"albumentations\", \"albumentations\"),\n",
    "    (\"pillow\", \"PIL\"),\n",
    "    (\"numpy\", \"numpy\"),\n",
    "    (\"scikit-learn\", \"sklearn\"),\n",
    "    (\"pandas\", \"pandas\"),\n",
    "    (\"tqdm\", \"tqdm\"),\n",
    "    (\"requests\", \"requests\"),\n",
    "    (\"datasets\", \"datasets\"),\n",
    "    (\"psutil\", \"psutil\"),  # For system memory info\n",
    "    (\"peft\", \"peft\"),  # For LoRA training\n",
    "    (\"transformers\", \"transformers\"),  # Required by peft\n",
    "]\n",
    "\n",
    "for pkg, imp in required_packages:\n",
    "    check_and_install_package(pkg, imp)\n",
    "\n",
    "# Verify GPU availability and system info\n",
    "import torch\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU Memory: {gpu_props.total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"GPU Compute Capability: {gpu_props.major}.{gpu_props.minor}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    \n",
    "    # RTX 8000 specific optimizations\n",
    "    if \"RTX 8000\" in gpu_name or \"8000\" in gpu_name:\n",
    "        print(\"\\nâœ“ RTX 8000 detected - Optimizing for 48GB VRAM\")\n",
    "        # RTX 8000 has 48GB VRAM, can handle larger batch sizes\n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "else:\n",
    "    print(\"âš  No GPU detected - will use CPU\")\n",
    "\n",
    "# Check available memory\n",
    "import psutil\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"\\nSystem RAM: {mem.total / 1e9:.2f} GB\")\n",
    "print(f\"Available RAM: {mem.available / 1e9:.2f} GB\")\n",
    "\n",
    "# Check disk space\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DISK SPACE CHECK\")\n",
    "print(\"=\"*60)\n",
    "disk = psutil.disk_usage('/')\n",
    "print(f\"Total disk: {disk.total / 1e9:.2f} GB\")\n",
    "print(f\"Used disk: {disk.used / 1e9:.2f} GB\")\n",
    "print(f\"Free disk: {disk.free / 1e9:.2f} GB\")\n",
    "print(f\"Usage: {disk.percent:.1f}%\")\n",
    "\n",
    "# Check home directory space\n",
    "home_disk = psutil.disk_usage(str(Path.home()))\n",
    "print(f\"\\nHome directory:\")\n",
    "print(f\"  Total: {home_disk.total / 1e9:.2f} GB\")\n",
    "print(f\"  Used: {home_disk.used / 1e9:.2f} GB\")\n",
    "print(f\"  Free: {home_disk.free / 1e9:.2f} GB\")\n",
    "print(f\"  Usage: {home_disk.percent:.1f}%\")\n",
    "\n",
    "# Check current directory space\n",
    "try:\n",
    "    cwd_disk = psutil.disk_usage(str(Path.cwd()))\n",
    "    print(f\"\\nCurrent directory:\")\n",
    "    print(f\"  Total: {cwd_disk.total / 1e9:.2f} GB\")\n",
    "    print(f\"  Used: {cwd_disk.used / 1e9:.2f} GB\")\n",
    "    print(f\"  Free: {cwd_disk.free / 1e9:.2f} GB\")\n",
    "    print(f\"  Usage: {cwd_disk.percent:.1f}%\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Warning if low on space\n",
    "min_space_gb = 10  # Minimum recommended free space in GB\n",
    "if disk.free / 1e9 < min_space_gb:\n",
    "    print(f\"\\nâš  WARNING: Low disk space! Only {disk.free / 1e9:.2f} GB free.\")\n",
    "    print(\"Consider:\")\n",
    "    print(\"  1. Using scratch/tmp directories (see next cell)\")\n",
    "    print(\"  2. Cleaning up old files\")\n",
    "    print(\"  3. Downloading only essential datasets\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ Sufficient disk space available ({disk.free / 1e9:.2f} GB free)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "\n",
    "# Determine user-writable directory\n",
    "# Try common supercomputer/user directories (prioritize scratch/tmp for more space)\n",
    "user_home = Path.home()\n",
    "possible_dirs = [\n",
    "    Path(\"/scratch\") / os.getenv(\"USER\", \"user\"),  # Scratch usually has more space\n",
    "    Path(\"/tmp\") / os.getenv(\"USER\", \"user\"),      # Temp often has more space\n",
    "    user_home / \"scratch\",                         # User scratch\n",
    "    user_home / \"work\",\n",
    "    user_home / \"tmp\",\n",
    "    Path.cwd(),  # Current working directory (last resort)\n",
    "]\n",
    "\n",
    "# Check disk space for each directory and prefer ones with more space\n",
    "def get_dir_space(path):\n",
    "    \"\"\"Get free space for a directory\"\"\"\n",
    "    try:\n",
    "        return psutil.disk_usage(str(path)).free\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Sort by available space (descending)\n",
    "possible_dirs_with_space = [(d, get_dir_space(d)) for d in possible_dirs]\n",
    "possible_dirs_with_space.sort(key=lambda x: x[1], reverse=True)\n",
    "possible_dirs = [d for d, _ in possible_dirs_with_space]\n",
    "\n",
    "# Show available directories with their space\n",
    "print(\"=\"*60)\n",
    "print(\"CHECKING AVAILABLE DIRECTORIES\")\n",
    "print(\"=\"*60)\n",
    "for dir_path, space in possible_dirs_with_space:\n",
    "    space_gb = space / 1e9\n",
    "    try:\n",
    "        # Test if writable\n",
    "        test_file = dir_path / \".write_test\"\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        test_file.write_text(\"test\")\n",
    "        test_file.unlink()\n",
    "        writable = \"âœ“ Writable\"\n",
    "    except:\n",
    "        writable = \"âœ— Not writable\"\n",
    "    print(f\"{str(dir_path):40s} {space_gb:8.2f} GB  {writable}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find first writable directory with sufficient space (prefer >10GB free)\n",
    "WORK_DIR = None\n",
    "min_space_gb = 10  # Minimum recommended space\n",
    "\n",
    "for dir_path, space_gb in possible_dirs_with_space:\n",
    "    space_gb_value = space_gb / 1e9\n",
    "    try:\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        # Test write\n",
    "        test_file = dir_path / \".write_test\"\n",
    "        test_file.write_text(\"test\")\n",
    "        test_file.unlink()\n",
    "        \n",
    "        # Prefer directories with more space\n",
    "        if space_gb_value >= min_space_gb:\n",
    "            WORK_DIR = dir_path / \"D-en-ominators\"\n",
    "            print(f\"\\nâœ“ Selected directory with {space_gb_value:.2f} GB free: {dir_path}\")\n",
    "            break\n",
    "    except (PermissionError, OSError):\n",
    "        continue\n",
    "\n",
    "# If no directory with sufficient space, use the one with most space\n",
    "if WORK_DIR is None:\n",
    "    for dir_path, space_gb in possible_dirs_with_space:\n",
    "        space_gb_value = space_gb / 1e9\n",
    "        try:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "            test_file = dir_path / \".write_test\"\n",
    "            test_file.write_text(\"test\")\n",
    "            test_file.unlink()\n",
    "            WORK_DIR = dir_path / \"D-en-ominators\"\n",
    "            print(f\"\\nâš  Using directory with {space_gb_value:.2f} GB free (less than recommended {min_space_gb} GB): {dir_path}\")\n",
    "            print(\"   Consider cleaning up or using cleanup functions later.\")\n",
    "            break\n",
    "        except (PermissionError, OSError):\n",
    "            continue\n",
    "\n",
    "if WORK_DIR is None:\n",
    "    # Fallback to current directory\n",
    "    WORK_DIR = Path.cwd() / \"D-en-ominators\"\n",
    "    print(f\"\\nâš  Using current directory as fallback: {WORK_DIR}\")\n",
    "\n",
    "# Set up directories\n",
    "NEW_DIR = WORK_DIR / \"new\"\n",
    "DATASETS_DIR = WORK_DIR / \"datasets\"\n",
    "CHECKPOINTS_DIR = WORK_DIR / \"checkpoints\"\n",
    "\n",
    "# Create directories\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "NEW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DATASETS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Change to working directory\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "# Add new folder to Python path\n",
    "sys.path.insert(0, str(NEW_DIR))\n",
    "sys.path.insert(0, str(WORK_DIR))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DIRECTORY SETUP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"âœ“ Working directory: {WORK_DIR}\")\n",
    "print(f\"âœ“ Code directory: {NEW_DIR}\")\n",
    "print(f\"âœ“ Datasets directory: {DATASETS_DIR}\")\n",
    "print(f\"âœ“ Checkpoints directory: {CHECKPOINTS_DIR}\")\n",
    "print(f\"âœ“ Current working directory: {Path.cwd()}\")\n",
    "\n",
    "# Check space in selected directory\n",
    "try:\n",
    "    work_disk = psutil.disk_usage(str(WORK_DIR.parent))\n",
    "    print(f\"\\nâœ“ Available space in {WORK_DIR.parent}: {work_disk.free / 1e9:.2f} GB\")\n",
    "    if work_disk.free / 1e9 < 5:\n",
    "        print(\"  âš  WARNING: Low space! Consider using cleanup functions or downloading fewer datasets.\")\n",
    "except:\n",
    "    pass\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository from GitHub\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "clone_dir = WORK_DIR.parent / \"D-en-ominators_repo\"\n",
    "if clone_dir.exists():\n",
    "    print(f\"âš  Repository directory exists, removing: {clone_dir}\")\n",
    "    shutil.rmtree(clone_dir)\n",
    "\n",
    "print(f\"Cloning repository to: {clone_dir}\")\n",
    "result = subprocess.run(\n",
    "    [\"git\", \"clone\", \"-b\", \"dev\", \"https://github.com/pragundamani/D-en-ominators.git\", str(clone_dir)],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"âœ“ Repository cloned successfully\")\n",
    "    \n",
    "    # Copy code files from the 'new' folder\n",
    "    source_new = clone_dir / \"new\"\n",
    "    if source_new.exists():\n",
    "        import glob\n",
    "        for file in glob.glob(str(source_new / \"*\")):\n",
    "            if os.path.isfile(file):\n",
    "                shutil.copy2(file, NEW_DIR)\n",
    "        print(f\"âœ“ Copied code files from {source_new} to {NEW_DIR}\")\n",
    "    else:\n",
    "        print(f\"âš  Warning: {source_new} not found after cloning\")\n",
    "else:\n",
    "    print(f\"âœ— Error cloning repository: {result.stderr}\")\n",
    "    print(\"You may need to upload files manually (Option B)\")\n",
    "\n",
    "print(f\"âœ“ Setup complete. Working directory: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MANUAL FILE UPLOAD\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Upload directory: {NEW_DIR}\")\n",
    "print(\"\\nInstructions:\")\n",
    "print(\"1. Upload a zip file containing all files from the 'new' folder\")\n",
    "print(\"2. Or upload individual Python files\")\n",
    "print(\"3. Files should be uploaded to the current working directory\")\n",
    "print(f\"\\nExpected upload location: {Path.cwd()}\")\n",
    "\n",
    "# Check for uploaded files in current directory\n",
    "uploaded_files = list(Path.cwd().glob(\"*.zip\")) + list(Path.cwd().glob(\"*.py\"))\n",
    "\n",
    "if uploaded_files:\n",
    "    print(f\"\\nâœ“ Found {len(uploaded_files)} uploaded file(s):\")\n",
    "    for f in uploaded_files:\n",
    "        print(f\"  - {f.name}\")\n",
    "    \n",
    "    # Process uploaded files\n",
    "    for filename in uploaded_files:\n",
    "        if filename.suffix == '.zip':\n",
    "            print(f\"\\nExtracting {filename.name}...\")\n",
    "            with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "                zip_ref.extractall(NEW_DIR)\n",
    "            print(f\"âœ“ Extracted {filename.name} to {NEW_DIR}\")\n",
    "        elif filename.suffix == '.py':\n",
    "            shutil.copy2(filename, NEW_DIR / filename.name)\n",
    "            print(f\"âœ“ Copied {filename.name} to {NEW_DIR}\")\n",
    "else:\n",
    "    print(\"\\nâš  No uploaded files found in current directory\")\n",
    "    print(\"Please upload files to:\", Path.cwd())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "if NEW_DIR.exists():\n",
    "    files_list = list(NEW_DIR.glob(\"*.py\"))\n",
    "    print(f\"Found {len(files_list)} Python files in {NEW_DIR}\")\n",
    "    for f in files_list[:5]:\n",
    "        print(f\"  âœ“ {f.name}\")\n",
    "    if len(files_list) > 5:\n",
    "        print(f\"  ... and {len(files_list) - 5} more files\")\n",
    "else:\n",
    "    print(f\"âš  {NEW_DIR} does not exist yet\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import psutil\n",
    "\n",
    "# Check disk space before starting\n",
    "def check_disk_space(path, required_gb=5):\n",
    "    \"\"\"Check if there's enough disk space\"\"\"\n",
    "    try:\n",
    "        disk = psutil.disk_usage(str(path))\n",
    "        free_gb = disk.free / 1e9\n",
    "        if free_gb < required_gb:\n",
    "            print(f\"âš  WARNING: Low disk space: {free_gb:.2f} GB free (need ~{required_gb} GB)\")\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return True  # Assume OK if can't check\n",
    "\n",
    "# Create datasets directory\n",
    "DATASETS_DIR = Path(\"datasets\")\n",
    "DATASETS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Check disk space\n",
    "if not check_disk_space(DATASETS_DIR, required_gb=10):\n",
    "    print(\"\\nâš  Consider cleaning up or using a different directory with more space\")\n",
    "    print(\"You can modify DATASETS_DIR to point to a scratch directory\")\n",
    "\n",
    "def run_command(cmd, check=True, show_progress=False):\n",
    "    \"\"\"Run a shell command and return the result\"\"\"\n",
    "    try:\n",
    "        if show_progress:\n",
    "            process = subprocess.Popen(\n",
    "                cmd, shell=True, stdout=subprocess.PIPE, \n",
    "                stderr=subprocess.STDOUT, text=True, bufsize=1\n",
    "            )\n",
    "            for line in process.stdout:\n",
    "                print(f\"  {line.strip()}\")\n",
    "            process.wait()\n",
    "            if process.returncode != 0:\n",
    "                print(f\"  âœ— Command failed with exit code {process.returncode}\")\n",
    "                return False\n",
    "            return True\n",
    "        else:\n",
    "            result = subprocess.run(cmd, shell=True, check=check, capture_output=True, text=True)\n",
    "            if result.returncode != 0 and check:\n",
    "                print(f\"  âœ— Command failed: {result.stderr}\")\n",
    "                return False\n",
    "            return result.returncode == 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error running command: {cmd}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        return False\n",
    "\n",
    "def dataset_exists(output_dir):\n",
    "    \"\"\"Check if dataset already exists and has content\"\"\"\n",
    "    output_path = DATASETS_DIR / output_dir\n",
    "    if output_path.exists() and any(output_path.rglob(\"*\")):\n",
    "        # Check if it has meaningful content (more than just empty dirs)\n",
    "        files = list(output_path.rglob(\"*\"))\n",
    "        non_empty = [f for f in files if f.is_file() and f.stat().st_size > 0]\n",
    "        return len(non_empty) > 0\n",
    "    return False\n",
    "\n",
    "def download_huggingface_dataset(dataset_name, output_dir):\n",
    "    \"\"\"Download a Hugging Face dataset\"\"\"\n",
    "    # Check if already exists\n",
    "    if dataset_exists(output_dir):\n",
    "        print(f\"\\nâ­ï¸  Skipping {dataset_name} - already exists in {output_dir}\")\n",
    "        return True\n",
    "    \n",
    "    print(f\"\\nðŸ“¦ Downloading Hugging Face dataset: {dataset_name}\")\n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        output_path = DATASETS_DIR / output_dir\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with tqdm(desc=\"  Loading dataset\", bar_format='{desc}: {elapsed}') as pbar:\n",
    "            print(f\"  Loading dataset {dataset_name}...\")\n",
    "            dataset = load_dataset(dataset_name)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        if isinstance(dataset, dict):\n",
    "            splits = list(dataset.keys())\n",
    "            with tqdm(total=len(splits), desc=\"  Saving splits\", unit=\"split\") as pbar:\n",
    "                for split_name, split_data in dataset.items():\n",
    "                    split_path = output_path / split_name\n",
    "                    split_path.mkdir(exist_ok=True)\n",
    "                    split_data.save_to_disk(str(split_path))\n",
    "                    pbar.set_postfix({\"split\": split_name})\n",
    "                    pbar.update(1)\n",
    "                    print(f\"  âœ“ Saved {split_name} split\")\n",
    "        else:\n",
    "            with tqdm(desc=\"  Saving dataset\", bar_format='{desc}: {elapsed}') as pbar:\n",
    "                dataset.save_to_disk(str(output_path))\n",
    "                pbar.update(1)\n",
    "                print(f\"  âœ“ Saved dataset\")\n",
    "        \n",
    "        if not any(output_path.rglob(\"*\")):\n",
    "            print(f\"  âœ— Warning: Dataset saved but no files found in {output_path}\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"  âœ“ Verified files saved to {output_path}\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(\"  âš  Installing datasets library...\")\n",
    "        with tqdm(desc=\"  Installing\", bar_format='{desc}: {elapsed}') as pbar:\n",
    "            run_command(f\"{sys.executable} -m pip install datasets\", check=False)\n",
    "            pbar.update(1)\n",
    "        return download_huggingface_dataset(dataset_name, output_dir)\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def download_github_repo(repo_url, output_dir, subpath=None):\n",
    "    \"\"\"Download a GitHub repository or specific subdirectory\"\"\"\n",
    "    # Check if already exists\n",
    "    if dataset_exists(output_dir):\n",
    "        print(f\"\\nâ­ï¸  Skipping {repo_url} - already exists in {output_dir}\")\n",
    "        return True\n",
    "    \n",
    "    print(f\"\\nðŸ“¦ Downloading GitHub repository: {repo_url}\")\n",
    "    try:\n",
    "        output_path = DATASETS_DIR / output_dir\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if \"github.com\" in repo_url:\n",
    "            repo_path = repo_url.split(\"github.com/\")[1].split(\"/tree/\")[0]\n",
    "            if \"/tree/\" in repo_url:\n",
    "                branch = repo_url.split(\"/tree/\")[1].split(\"/\")[0]\n",
    "                subpath = \"/\".join(repo_url.split(\"/tree/\")[1].split(\"/\")[1:])\n",
    "            else:\n",
    "                branch = \"main\"\n",
    "            \n",
    "            repo_url_clean = f\"https://github.com/{repo_path}.git\"\n",
    "            \n",
    "            print(f\"  Cloning {repo_url_clean} (branch: {branch})...\")\n",
    "            with tqdm(desc=\"  Cloning repository\", bar_format='{desc}: {elapsed}') as pbar:\n",
    "                success = run_command(f\"git clone --depth 1 --branch {branch} {repo_url_clean} {output_path}\", \n",
    "                          check=False, show_progress=True)\n",
    "                pbar.update(1)\n",
    "            \n",
    "            if not success:\n",
    "                print(f\"  âœ— Failed to clone repository\")\n",
    "                return False\n",
    "            \n",
    "            if not output_path.exists() or not any(output_path.iterdir()):\n",
    "                print(f\"  âœ— Clone completed but directory is empty\")\n",
    "                return False\n",
    "            \n",
    "            if subpath:\n",
    "                subpath_full = output_path / subpath\n",
    "                if subpath_full.exists():\n",
    "                    print(f\"  âœ“ Found subpath: {subpath}\")\n",
    "                else:\n",
    "                    print(f\"  âš  Subpath {subpath} not found\")\n",
    "            \n",
    "            print(f\"  âœ“ Successfully cloned to {output_path}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"  âœ— Invalid GitHub URL: {repo_url}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def download_kaggle_dataset(dataset_name, output_dir):\n",
    "    \"\"\"Download a Kaggle dataset\"\"\"\n",
    "    # Check if already exists\n",
    "    if dataset_exists(output_dir):\n",
    "        print(f\"\\nâ­ï¸  Skipping {dataset_name} - already exists in {output_dir}\")\n",
    "        return True\n",
    "    \n",
    "    print(f\"\\nðŸ“¦ Downloading Kaggle dataset: {dataset_name}\")\n",
    "    try:\n",
    "        output_path = DATASETS_DIR / output_dir\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            import kaggle\n",
    "        except ImportError:\n",
    "            print(\"  âš  Installing kaggle library...\")\n",
    "            with tqdm(desc=\"  Installing\", bar_format='{desc}: {elapsed}') as pbar:\n",
    "                run_command(f\"{sys.executable} -m pip install kaggle\", check=False)\n",
    "                pbar.update(1)\n",
    "            import kaggle\n",
    "        \n",
    "        print(f\"  Downloading {dataset_name}...\")\n",
    "        with tqdm(desc=\"  Downloading from Kaggle\", bar_format='{desc}: {elapsed}') as pbar:\n",
    "            kaggle.api.dataset_download_files(dataset_name, path=str(output_path), unzip=True)\n",
    "            pbar.update(1)\n",
    "        \n",
    "        if not any(output_path.rglob(\"*\")):\n",
    "            print(f\"  âœ— Warning: Download completed but no files found in {output_path}\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"  âœ“ Downloaded to {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(f\"  âš  Note: Kaggle requires API credentials. Upload kaggle.json to /content/.kaggle/\")\n",
    "        return False\n",
    "\n",
    "def download_file(url, output_path):\n",
    "    \"\"\"Download a file directly from URL\"\"\"\n",
    "    output_file = DATASETS_DIR / output_path\n",
    "    \n",
    "    # Check if already exists\n",
    "    if output_file.exists() and output_file.stat().st_size > 0:\n",
    "        print(f\"\\nâ­ï¸  Skipping {url} - file already exists: {output_file}\")\n",
    "        return True\n",
    "    \n",
    "    print(f\"\\nðŸ“¦ Downloading file: {url}\")\n",
    "    try:\n",
    "        output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        response = requests.get(url, stream=True, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        with open(output_file, 'wb') as f:\n",
    "            if total_size > 0:\n",
    "                with tqdm(total=total_size, unit='B', unit_scale=True, unit_divisor=1024, \n",
    "                      desc=\"  Downloading\", bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]') as pbar:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                            pbar.update(len(chunk))\n",
    "            else:\n",
    "                with tqdm(unit='B', unit_scale=True, unit_divisor=1024, \n",
    "                         desc=\"  Downloading\", bar_format='{desc}: {n_fmt} [{elapsed}]') as pbar:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk)\n",
    "                            pbar.update(len(chunk))\n",
    "        \n",
    "        print(f\"  âœ“ Downloaded to {output_file}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"\\n  âœ— Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# List of datasets to download with priority levels\n",
    "# ESSENTIAL = Always download (needed for training)\n",
    "# OPTIONAL = Download only if space allows\n",
    "\n",
    "datasets = [\n",
    "    {\n",
    "        \"name\": \"Homogenized Finalized Dataset\",\n",
    "        \"type\": \"github\",\n",
    "        \"url\": \"https://github.com/pragundamani/D-en-ominators/tree/dev/finalized_dataset/homogenized\",\n",
    "        \"output\": \"homogenized_dataset\",\n",
    "        \"subpath\": \"finalized_dataset/homogenized\",\n",
    "        \"estimated_size_gb\": 3\n",
    "    }\n",
    "] dataset['output'])\n",
    "        elif dataset['type'] == 'github':\n",
    "            success = download_github_repo(dataset['url'], dataset['output'], dataset.get('subpath'))\n",
    "        elif dataset['type'] == 'kaggle':\n",
    "            success = download_kaggle_dataset(dataset['dataset_id'], dataset['output'])\n",
    "        elif dataset['type'] == 'direct':\n",
    "            success = download_file(dataset['url'], dataset['output'])\n",
    "        \n",
    "        results.append({\n",
    "            \"name\": dataset['name'],\n",
    "            \"success\": success\n",
    "        })\n",
    "        \n",
    "        overall_pbar.update(1)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DOWNLOAD SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "for result in results:\n",
    "    status = \"âœ“\" if result['success'] else \"âœ—\"\n",
    "    print(f\"{status} {result['name']}\")\n",
    "\n",
    "successful = sum(1 for r in results if r['success'])\n",
    "# Create a mapping of dataset names to output directories\n",
    "dataset_map = {d['name']: d['output'] for d in datasets}\n",
    "skipped = sum(1 for r in results if r['success'] and dataset_exists(dataset_map.get(r['name'], '')))\n",
    "print(f\"\\nSuccessfully processed: {successful}/{len(results)} datasets\")\n",
    "if skipped > 0:\n",
    "    print(f\"Skipped (already existed): {skipped} datasets\")\n",
    "print(f\"Datasets saved to: {DATASETS_DIR.absolute()}\")\n",
    "\n",
    "# Check disk space after downloads\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DISK SPACE AFTER DOWNLOADS\")\n",
    "print(\"=\"*60)\n",
    "try:\n",
    "    disk = psutil.disk_usage(str(DATASETS_DIR))\n",
    "    print(f\"Free space: {disk.free / 1e9:.2f} GB\")\n",
    "    if disk.free / 1e9 < 5:\n",
    "        print(\"âš  WARNING: Low disk space remaining!\")\n",
    "except:\n",
    "    pass\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup functions to free disk space\n",
    "\n",
    "def cleanup_old_checkpoints(keep_best=True, keep_latest=3):\n",
    "    \"\"\"Remove old checkpoints, keeping only best and latest N\"\"\"\n",
    "    if not CHECKPOINTS_DIR.exists():\n",
    "        return 0\n",
    "    \n",
    "    checkpoints = list(CHECKPOINTS_DIR.glob(\"*.pth\"))\n",
    "    if len(checkpoints) <= keep_latest + (1 if keep_best else 0):\n",
    "        print(\"No old checkpoints to remove\")\n",
    "        return 0\n",
    "    \n",
    "    # Keep best model\n",
    "    best_model = CHECKPOINTS_DIR / \"best_model.pth\"\n",
    "    kept = [best_model] if keep_best and best_model.exists() else []\n",
    "    \n",
    "    # Sort by modification time, keep latest N\n",
    "    checkpoints.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    kept.extend(checkpoints[:keep_latest])\n",
    "    \n",
    "    # Remove others\n",
    "    removed_size = 0\n",
    "    for cp in checkpoints:\n",
    "        if cp not in kept:\n",
    "            size = cp.stat().st_size\n",
    "            cp.unlink()\n",
    "            removed_size += size\n",
    "            print(f\"  Removed: {cp.name} ({size / 1e6:.2f} MB)\")\n",
    "    \n",
    "    print(f\"âœ“ Freed {removed_size / 1e9:.2f} GB from old checkpoints\")\n",
    "    return removed_size\n",
    "\n",
    "def cleanup_temporary_files():\n",
    "    \"\"\"Remove temporary files and caches\"\"\"\n",
    "    removed_size = 0\n",
    "    \n",
    "    # Remove __pycache__\n",
    "    for pycache in WORK_DIR.rglob(\"__pycache__\"):\n",
    "        if pycache.is_dir():\n",
    "            size = sum(f.stat().st_size for f in pycache.rglob(\"*\") if f.is_file())\n",
    "            shutil.rmtree(pycache)\n",
    "            removed_size += size\n",
    "            print(f\"  Removed: {pycache}\")\n",
    "    \n",
    "    # Remove .pyc files\n",
    "    for pyc in WORK_DIR.rglob(\"*.pyc\"):\n",
    "        size = pyc.stat().st_size\n",
    "        pyc.unlink()\n",
    "        removed_size += size\n",
    "    \n",
    "    # Remove .DS_Store files\n",
    "    for ds_store in WORK_DIR.rglob(\".DS_Store\"):\n",
    "        size = ds_store.stat().st_size\n",
    "        ds_store.unlink()\n",
    "        removed_size += size\n",
    "    \n",
    "    print(f\"âœ“ Freed {removed_size / 1e6:.2f} MB from temporary files\")\n",
    "    return removed_size\n",
    "\n",
    "def cleanup_dataset_cache():\n",
    "    \"\"\"Remove Hugging Face cache if needed\"\"\"\n",
    "    hf_cache = Path.home() / \".cache\" / \"huggingface\"\n",
    "    if hf_cache.exists():\n",
    "        size = sum(f.stat().st_size for f in hf_cache.rglob(\"*\") if f.is_file())\n",
    "        print(f\"âš  Hugging Face cache found: {size / 1e9:.2f} GB\")\n",
    "        response = input(\"Remove Hugging Face cache? (y/n): \").lower()\n",
    "        if response == 'y':\n",
    "            shutil.rmtree(hf_cache)\n",
    "            print(f\"âœ“ Freed {size / 1e9:.2f} GB\")\n",
    "            return size\n",
    "    return 0\n",
    "\n",
    "def show_disk_usage():\n",
    "    \"\"\"Show disk usage by directory\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"DISK USAGE BREAKDOWN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    dirs_to_check = {\n",
    "        \"Datasets\": DATASETS_DIR,\n",
    "        \"Checkpoints\": CHECKPOINTS_DIR,\n",
    "        \"Code\": NEW_DIR,\n",
    "        \"Working Directory\": WORK_DIR,\n",
    "    }\n",
    "    \n",
    "    total_size = 0\n",
    "    for name, path in dirs_to_check.items():\n",
    "        if path.exists():\n",
    "            size = sum(f.stat().st_size for f in path.rglob(\"*\") if f.is_file())\n",
    "            total_size += size\n",
    "            print(f\"{name:20s}: {size / 1e9:8.2f} GB\")\n",
    "    \n",
    "    print(f\"{'Total':20s}: {total_size / 1e9:8.2f} GB\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run cleanup\n",
    "print(\"=\"*60)\n",
    "print(\"CLEANUP OPTIONS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Uncomment the cleanup functions you want to run:\\n\")\n",
    "\n",
    "# Uncomment to run cleanups:\n",
    "# cleanup_old_checkpoints(keep_best=True, keep_latest=3)\n",
    "# cleanup_temporary_files()\n",
    "# cleanup_dataset_cache()  # Interactive - asks for confirmation\n",
    "# show_disk_usage()\n",
    "\n",
    "print(\"\\nTo free up space, uncomment the cleanup functions above and run this cell.\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "print(\"Checking setup...\")\n",
    "print(\"\\n1. Code files:\")\n",
    "required_files = ['hieroglyph_dataset.py', 'model.py', 'train.py', 'inference.py']\n",
    "for f in required_files:\n",
    "    file_path = NEW_DIR / f\n",
    "    status = \"âœ“\" if file_path.exists() else \"âœ—\"\n",
    "    print(f\"  {status} {f}\")\n",
    "\n",
    "print(\"\\n2. Datasets:\")\n",
    "if DATASETS_DIR.exists():\n",
    "    dataset_dirs = [d for d in DATASETS_DIR.iterdir() if d.is_dir()]\n",
    "    print(f\"  âœ“ Found {len(dataset_dirs)} dataset directories\")\n",
    "    for d in dataset_dirs[:5]:\n",
    "        print(f\"    - {d.name}\")\n",
    "else:\n",
    "    print(\"  âœ— Datasets directory not found\")\n",
    "\n",
    "print(\"\\n3. Python path:\")\n",
    "print(f\"  âœ“ NEW_DIR in path: {str(NEW_DIR) in sys.path}\")\n",
    "\n",
    "print(\"\\n4. GPU:\")\n",
    "import torch\n",
    "print(f\"  {'âœ“' if torch.cuda.is_available() else 'âœ—'} CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preload dataset into RAM for faster training\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Find the dataset paths\n",
    "json_path = None\n",
    "images_dir = None\n",
    "\n",
    "for json_file in DATASETS_DIR.rglob(\"gardiner_hieroglyphs_with_unicode_hex.json\"):\n",
    "    json_path = str(json_file)\n",
    "    utf_pngs = json_file.parent / \"utf-pngs\"\n",
    "    if utf_pngs.exists():\n",
    "        images_dir = str(utf_pngs)\n",
    "    break\n",
    "\n",
    "# Check if we should use RAM caching\n",
    "USE_RAM_CACHE = True  # Set to False to disable RAM caching\n",
    "RAM_CACHE_FILE = WORK_DIR / \"dataset_ram_cache.pt\"\n",
    "\n",
    "def estimate_dataset_size(images_dir):\n",
    "    \"\"\"Estimate dataset size in memory\"\"\"\n",
    "    image_files = list(Path(images_dir).glob(\"*.png\"))\n",
    "    if not image_files:\n",
    "        return 0\n",
    "    \n",
    "    # Sample a few images to estimate\n",
    "    sample_size = min(10, len(image_files))\n",
    "    total_size = 0\n",
    "    for img_file in image_files[:sample_size]:\n",
    "        try:\n",
    "            img = Image.open(img_file)\n",
    "            # Estimate: width * height * 3 channels * 4 bytes (float32) * 2 (augmentation overhead)\n",
    "            size = img.size[0] * img.size[1] * 3 * 4 * 2\n",
    "            total_size += size\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    avg_size = (total_size / sample_size) if sample_size > 0 else 0\n",
    "    estimated_gb = (avg_size * len(image_files)) / 1e9\n",
    "    return estimated_gb\n",
    "\n",
    "def preload_dataset_to_ram(json_path, images_dir, cache_file=None):\n",
    "    \"\"\"Preload all images into RAM\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"PRELOADING DATASET INTO RAM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check if cache exists\n",
    "    if cache_file and cache_file.exists():\n",
    "        print(f\"âœ“ Found existing RAM cache: {cache_file}\")\n",
    "        response = input(\"Load from cache? (y/n, default=y): \").lower().strip()\n",
    "        if response != 'n':\n",
    "            print(\"Loading dataset from RAM cache...\")\n",
    "            cache_data = torch.load(cache_file, map_location='cpu')\n",
    "            print(f\"âœ“ Loaded {len(cache_data['images'])} images from cache\")\n",
    "            print(f\"  Memory usage: ~{cache_data.get('memory_gb', 0):.2f} GB\")\n",
    "            return cache_data\n",
    "    \n",
    "    # Load JSON metadata\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        hieroglyph_data = json.load(f)\n",
    "    \n",
    "    # Estimate memory needed\n",
    "    estimated_gb = estimate_dataset_size(images_dir)\n",
    "    print(f\"Estimated dataset size: ~{estimated_gb:.2f} GB in RAM\")\n",
    "    \n",
    "    # Check available RAM and GPU VRAM (optimize for 64GB RAM + 48GB VRAM RTX 8000)\n",
    "    mem = psutil.virtual_memory()\n",
    "    total_ram_gb = mem.total / 1e9\n",
    "    available_ram_gb = mem.available / 1e9\n",
    "    print(f\"Total RAM: {total_ram_gb:.2f} GB\")\n",
    "    print(f\"Available RAM: {available_ram_gb:.2f} GB\")\n",
    "    \n",
    "    # Check GPU VRAM if available\n",
    "    gpu_vram_gb = 0\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_props = torch.cuda.get_device_properties(0)\n",
    "        gpu_vram_gb = gpu_props.total_memory / 1e9\n",
    "        print(f\"GPU VRAM: {gpu_vram_gb:.2f} GB (RTX 8000 has 48GB)\")\n",
    "        print(f\"  â†’ GPU VRAM will be used for model/batches during training\")\n",
    "        print(f\"  â†’ RAM will be used for dataset storage (faster than disk)\")\n",
    "    \n",
    "    # Use up to 90% of available RAM for 64GB systems (more aggressive for large RAM)\n",
    "    max_ram_usage = available_ram_gb * 0.9 if total_ram_gb >= 60 else available_ram_gb * 0.8\n",
    "    \n",
    "    if estimated_gb > max_ram_usage:\n",
    "        if total_ram_gb >= 60:\n",
    "            print(f\"âš  Dataset ({estimated_gb:.2f} GB) exceeds safe limit ({max_ram_usage:.2f} GB)\")\n",
    "            print(\"  âœ“ 64GB RAM detected - proceeding with preload (will use available RAM)\")\n",
    "        else:\n",
    "            print(f\"âš  WARNING: Dataset may exceed available RAM!\")\n",
    "            response = input(\"Continue anyway? (y/n): \").lower().strip()\n",
    "            if response != 'y':\n",
    "                print(\"Skipping RAM preload. Will use disk-based loading.\")\n",
    "                return None\n",
    "    \n",
    "    # Load all images into memory (optimized for 64GB RAM with parallel loading)\n",
    "    images_dir_path = Path(images_dir)\n",
    "    image_files = list(images_dir_path.glob(\"*.png\"))\n",
    "    \n",
    "    print(f\"\\nLoading {len(image_files)} images into RAM (using 64GB RAM)...\")\n",
    "    print(\"  Using parallel loading for faster processing...\")\n",
    "    \n",
    "    images_dict = {}\n",
    "    metadata_dict = {}\n",
    "    failed = 0\n",
    "    \n",
    "    # Use multiprocessing for faster loading on systems with many cores\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    \n",
    "    def load_image(img_file):\n",
    "        \"\"\"Load a single image\"\"\"\n",
    "        try:\n",
    "            gardiner_num = img_file.stem\n",
    "            img = Image.open(img_file).convert('RGB')\n",
    "            img_array = np.array(img, dtype=np.uint8)\n",
    "            \n",
    "            # Get metadata\n",
    "            metadata = None\n",
    "            if gardiner_num in hieroglyph_data:\n",
    "                metadata = hieroglyph_data[gardiner_num]\n",
    "            \n",
    "            return gardiner_num, img_array, metadata, None\n",
    "        except Exception as e:\n",
    "            return None, None, None, str(e)\n",
    "    \n",
    "    # Load images with parallel processing (faster on multi-core systems)\n",
    "    max_workers = min(8, os.cpu_count() or 4)  # Use up to 8 workers\n",
    "    \n",
    "    with tqdm(total=len(image_files), desc=\"Loading images\", unit=\"img\") as pbar:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {executor.submit(load_image, img_file): img_file for img_file in image_files}\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                gardiner_num, img_array, metadata, error = future.result()\n",
    "                if gardiner_num and img_array is not None:\n",
    "                    images_dict[gardiner_num] = img_array\n",
    "                    if metadata:\n",
    "                        metadata_dict[gardiner_num] = metadata\n",
    "                else:\n",
    "                    failed += 1\n",
    "                pbar.update(1)\n",
    "    \n",
    "    # Calculate actual memory usage\n",
    "    total_memory = sum(img.nbytes for img in images_dict.values())\n",
    "    memory_gb = total_memory / 1e9\n",
    "    \n",
    "    cache_data = {\n",
    "        'images': images_dict,\n",
    "        'metadata': metadata_dict,\n",
    "        'json_path': json_path,\n",
    "        'images_dir': images_dir,\n",
    "        'memory_gb': memory_gb,\n",
    "        'num_images': len(images_dict)\n",
    "    }\n",
    "    \n",
    "    # Save cache for future use\n",
    "    if cache_file:\n",
    "        print(f\"\\nSaving RAM cache to {cache_file}...\")\n",
    "        torch.save(cache_data, cache_file)\n",
    "        print(\"âœ“ Cache saved\")\n",
    "    \n",
    "    print(f\"\\nâœ“ Preloaded {len(images_dict)} images into RAM\")\n",
    "    print(f\"  Memory usage: ~{memory_gb:.2f} GB\")\n",
    "    print(f\"  Failed to load: {failed} images\")\n",
    "    \n",
    "    # Show memory utilization\n",
    "    mem_after = psutil.virtual_memory()\n",
    "    print(f\"\\nMemory Status:\")\n",
    "    print(f\"  RAM used by dataset: {memory_gb:.2f} GB\")\n",
    "    print(f\"  Total RAM used: {(mem_after.used / 1e9):.2f} GB / {total_ram_gb:.2f} GB\")\n",
    "    print(f\"  Available RAM remaining: {(mem_after.available / 1e9):.2f} GB\")\n",
    "    \n",
    "    if torch.cuda.is_available() and gpu_vram_gb > 0:\n",
    "        print(f\"  GPU VRAM available: {gpu_vram_gb:.2f} GB (for model/batch data)\")\n",
    "        print(f\"  âœ“ Training will use GPU VRAM for model, RAM for data (optimal setup)\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return cache_data\n",
    "\n",
    "# Preload if enabled and dataset found\n",
    "ram_cache_data = None\n",
    "if USE_RAM_CACHE and json_path and images_dir:\n",
    "    ram_cache_data = preload_dataset_to_ram(json_path, images_dir, RAM_CACHE_FILE)\n",
    "    if ram_cache_data:\n",
    "        print(\"\\nâœ“ Dataset ready in RAM! Training will be much faster.\")\n",
    "    else:\n",
    "        print(\"\\nâš  RAM preload skipped or failed. Will use disk-based loading.\")\n",
    "elif not json_path or not images_dir:\n",
    "    print(\"âš  Dataset not found. Skipping RAM preload.\")\n",
    "else:\n",
    "    print(\"âš  RAM caching disabled. Set USE_RAM_CACHE = True to enable.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dataset paths (already found in previous cell if RAM cache was loaded)\n",
    "# If not found, search again\n",
    "if 'json_path' not in locals() or not json_path:\n",
    "    json_path = None\n",
    "    images_dir = None\n",
    "    \n",
    "    # Search for the JSON file\n",
    "    for json_file in DATASETS_DIR.rglob(\"gardiner_hieroglyphs_with_unicode_hex.json\"):\n",
    "        json_path = str(json_file)\n",
    "        # Look for utf-pngs in the same parent directory\n",
    "        utf_pngs = json_file.parent / \"utf-pngs\"\n",
    "        if utf_pngs.exists():\n",
    "            images_dir = str(utf_pngs)\n",
    "        break\n",
    "\n",
    "if json_path and images_dir:\n",
    "    print(f\"âœ“ Found JSON: {json_path}\")\n",
    "    print(f\"âœ“ Found images: {images_dir}\")\n",
    "    print(f\"âœ“ Checkpoints will be saved to: {CHECKPOINTS_DIR}\")\n",
    "    \n",
    "    # Check if RAM cache is available\n",
    "    use_ram = ram_cache_data is not None\n",
    "    if use_ram:\n",
    "        print(f\"âœ“ Using RAM cache: {ram_cache_data['num_images']} images (~{ram_cache_data.get('memory_gb', 0):.2f} GB)\")\n",
    "        print(\"  Training will be faster with in-memory data!\")\n",
    "    else:\n",
    "        print(\"  Using disk-based loading (slower but uses less RAM)\")\n",
    "    \n",
    "    print(\"\\nStarting LoRA training...\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"LoRA (Low-Rank Adaptation) Training\")\n",
    "    print(\"  - Only trains a small number of parameters (~1-5% of model)\")\n",
    "    print(\"  - Much faster training and lower memory usage\")\n",
    "    print(\"  - Maintains model performance\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Set environment variables for GPU\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" if torch.cuda.is_available() else \"\"\n",
    "    \n",
    "    # RTX 8000 optimizations - can handle larger batch sizes with 48GB VRAM\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        if \"RTX 8000\" in gpu_name or \"8000\" in gpu_name:\n",
    "            batch_size = \"64\"  # RTX 8000 can handle larger batches\n",
    "            print(\"âœ“ RTX 8000 detected - Using batch size 64 (48GB VRAM)\")\n",
    "        else:\n",
    "            batch_size = \"32\"  # Default for other GPUs\n",
    "    else:\n",
    "        batch_size = \"16\"  # Smaller for CPU\n",
    "    \n",
    "    # Write train_lora.py to NEW_DIR (embedded in notebook for HPC)\n",
    "    train_lora_path = NEW_DIR / \"train_lora.py\"\n",
    "    if not train_lora_path.exists():\n",
    "        print(f\"Creating train_lora.py in {NEW_DIR}...\")\n",
    "        \n",
    "        train_lora_code = '''\"\"\"\n",
    "LoRA Training Script for Hieroglyph Classifier\n",
    "Uses PEFT (Parameter-Efficient Fine-Tuning) with LoRA\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "from hieroglyph_dataset import HieroglyphDataset, get_transforms\n",
    "from model import create_model\n",
    "\n",
    "\n",
    "class LoRATrainer:\n",
    "    \"\"\"Training manager with LoRA for hieroglyph classifier.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        device: str = \"cuda\",\n",
    "        learning_rate: float = 1e-4,\n",
    "        weight_decay: float = 1e-4,\n",
    "        class_weights: Optional[torch.Tensor] = None,\n",
    "        use_focal_loss: bool = True,\n",
    "        focal_gamma: float = 2.0,\n",
    "        label_smoothing: float = 0.1\n",
    "    ):\n",
    "        # Apply LoRA to the model\n",
    "        print(\"\\\\n\" + \"=\"*60)\n",
    "        print(\"APPLYING LoRA TO MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # LoRA configuration\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.FEATURE_EXTRACTION,  # For vision models\n",
    "            r=16,  # Rank (low-rank dimension)\n",
    "            lora_alpha=32,  # LoRA alpha scaling\n",
    "            target_modules=[\"qkv\", \"proj\", \"fc1\", \"fc2\"],  # Target attention and MLP layers\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "        )\n",
    "        \n",
    "        # Convert model to PEFT with LoRA\n",
    "        self.model = get_peft_model(model, lora_config)\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters (LoRA): {trainable_params:,}\")\n",
    "        print(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.class_weights = class_weights\n",
    "        \n",
    "        # Loss function\n",
    "        if use_focal_loss:\n",
    "            self.criterion = FocalLoss(alpha=class_weights, gamma=focal_gamma)\n",
    "            print(f\"Using Focal Loss (gamma={focal_gamma})\")\n",
    "        else:\n",
    "            if class_weights is not None:\n",
    "                self.criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=label_smoothing)\n",
    "            else:\n",
    "                self.criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "        \n",
    "        # Optimizer - only optimize LoRA parameters\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(),  # Only LoRA parameters are trainable\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=3\n",
    "        )\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': []\n",
    "        }\n",
    "        \n",
    "        self.best_val_acc = 0.0\n",
    "        self.best_model_state = None\n",
    "    \n",
    "    def train_epoch(self) -> Dict[str, float]:\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=\"Training\")\n",
    "        for images, labels, _ in pbar:\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.train_loader)\n",
    "        epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "        \n",
    "        return {'loss': epoch_loss, 'acc': epoch_acc}\n",
    "    \n",
    "    def validate(self) -> Dict[str, float]:\n",
    "        \"\"\"Validate the model.\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(self.val_loader, desc=\"Validation\")\n",
    "            for images, labels, _ in pbar:\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.val_loader)\n",
    "        epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        \n",
    "        return {'loss': epoch_loss, 'acc': epoch_acc, 'f1': f1}\n",
    "    \n",
    "    def train(self, num_epochs: int, save_dir: Path) -> Dict:\n",
    "        \"\"\"Train the model.\"\"\"\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\"TRAINING WITH LoRA\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Epochs: {num_epochs}\")\n",
    "        print(f\"Save directory: {save_dir}\")\n",
    "        print(f\"{'='*60}\\\\n\")\n",
    "        \n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            # Train\n",
    "            train_metrics = self.train_epoch()\n",
    "            self.history['train_loss'].append(train_metrics['loss'])\n",
    "            self.history['train_acc'].append(train_metrics['acc'])\n",
    "            \n",
    "            # Validate\n",
    "            val_metrics = self.validate()\n",
    "            self.history['val_loss'].append(val_metrics['loss'])\n",
    "            self.history['val_acc'].append(val_metrics['acc'])\n",
    "            \n",
    "            # Update learning rate\n",
    "            self.scheduler.step(val_metrics['loss'])\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"Train Loss: {train_metrics['loss']:.4f}, Train Acc: {train_metrics['acc']:.4f}\")\n",
    "            print(f\"Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['acc']:.4f}, Val F1: {val_metrics['f1']:.4f}\")\n",
    "            print(f\"LR: {self.optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_metrics['acc'] > self.best_val_acc:\n",
    "                self.best_val_acc = val_metrics['acc']\n",
    "                self.best_model_state = self.model.state_dict()\n",
    "                \n",
    "                # Save LoRA adapters\n",
    "                best_path = save_dir / \"best_model_lora.pth\"\n",
    "                self.model.save_pretrained(str(save_dir / \"lora_adapters\"))\n",
    "                torch.save({\n",
    "                    'model_state_dict': self.best_model_state,\n",
    "                    'epoch': epoch,\n",
    "                    'val_acc': self.best_val_acc,\n",
    "                    'history': self.history\n",
    "                }, best_path)\n",
    "                print(f\"âœ“ Saved best model (val_acc: {self.best_val_acc:.4f})\")\n",
    "            \n",
    "            print()\n",
    "        \n",
    "        return self.history\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss for handling imbalanced classes.\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Train hieroglyph classifier with LoRA')\n",
    "    parser.add_argument('--json_path', type=str, required=True, help='Path to JSON metadata file')\n",
    "    parser.add_argument('--images_dir', type=str, required=True, help='Path to images directory')\n",
    "    parser.add_argument('--ram_cache', type=str, default=None, help='Path to RAM cache file')\n",
    "    parser.add_argument('--model_name', type=str, default='vit_base_patch16_224', help='Timm model name')\n",
    "    parser.add_argument('--batch_size', type=int, default=32, help='Batch size')\n",
    "    parser.add_argument('--num_epochs', type=int, default=50, help='Number of epochs')\n",
    "    parser.add_argument('--learning_rate', type=float, default=1e-4, help='Learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-4, help='Weight decay')\n",
    "    parser.add_argument('--val_split', type=float, default=0.2, help='Validation split ratio')\n",
    "    parser.add_argument('--save_dir', type=str, required=True, help='Directory to save checkpoints')\n",
    "    parser.add_argument('--use_focal_loss', action='store_true', help='Use focal loss')\n",
    "    parser.add_argument('--focal_gamma', type=float, default=2.0, help='Focal loss gamma')\n",
    "    parser.add_argument('--label_smoothing', type=float, default=0.1, help='Label smoothing')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    print(\"\\\\nLoading dataset...\")\n",
    "    if args.ram_cache and Path(args.ram_cache).exists():\n",
    "        print(f\"Loading from RAM cache: {args.ram_cache}\")\n",
    "        cache_data = torch.load(args.ram_cache, map_location='cpu')\n",
    "        # Use regular dataset but we'll need to modify it to use RAM cache\n",
    "        # For now, use regular dataset loading\n",
    "        full_dataset = HieroglyphDataset(\n",
    "            json_path=cache_data.get('json_path', args.json_path),\n",
    "            images_dir=cache_data.get('images_dir', args.images_dir),\n",
    "            transform=get_transforms(train=True)\n",
    "        )\n",
    "        print(\"Note: RAM cache loaded but dataset will load from disk. To use RAM cache, modify HieroglyphDataset.__getitem__\")\n",
    "    else:\n",
    "        full_dataset = HieroglyphDataset(\n",
    "            json_path=args.json_path,\n",
    "            images_dir=args.images_dir,\n",
    "            transform=get_transforms(train=True)\n",
    "        )\n",
    "    \n",
    "    # Split dataset\n",
    "    train_size = int((1 - args.val_split) * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        full_dataset, [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "    \n",
    "    # Compute class weights\n",
    "    train_labels = [full_dataset[i][1] for i in train_dataset.indices]\n",
    "    unique_labels = np.unique(train_labels)\n",
    "    class_weights = compute_class_weight('balanced', classes=unique_labels, y=train_labels)\n",
    "    label_to_weight = {label: weight for label, weight in zip(unique_labels, class_weights)}\n",
    "    class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "    \n",
    "    # Create weighted sampler\n",
    "    sample_weights = np.array([label_to_weight.get(label, 1.0) for label in train_labels])\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    print(\"\\\\nCreating model...\")\n",
    "    model = create_model(\n",
    "        model_name=args.model_name,\n",
    "        num_classes=full_dataset.num_classes,\n",
    "        pretrained=True,\n",
    "        dropout_rate=0.3,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Create LoRA trainer\n",
    "    trainer = LoRATrainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        learning_rate=args.learning_rate,\n",
    "        weight_decay=args.weight_decay,\n",
    "        class_weights=class_weights,\n",
    "        use_focal_loss=args.use_focal_loss,\n",
    "        focal_gamma=args.focal_gamma,\n",
    "        label_smoothing=args.label_smoothing if not args.use_focal_loss else 0.0\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    save_dir = Path(args.save_dir)\n",
    "    history = trainer.train(\n",
    "        num_epochs=args.num_epochs,\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "    \n",
    "    # Save training history\n",
    "    history_path = save_dir / \"training_history.json\"\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    print(f\"\\\\nTraining history saved to {history_path}\")\n",
    "    print(\"\\\\nTraining completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "        \n",
    "        with open(train_lora_path, 'w') as f:\n",
    "            f.write(train_lora_code)\n",
    "        print(\"âœ“ Created train_lora.py (embedded in notebook)\")\n",
    "    \n",
    "    # Create a wrapper script that uses RAM cache if available\n",
    "    if use_ram:\n",
    "        # Save RAM cache path for training script\n",
    "        ram_cache_env = str(RAM_CACHE_FILE)\n",
    "        os.environ[\"RAM_CACHE_PATH\"] = ram_cache_env\n",
    "        print(f\"âœ“ RAM cache path set: {ram_cache_env}\")\n",
    "        \n",
    "        train_cmd = [\n",
    "            sys.executable,\n",
    "            str(train_lora_path),\n",
    "            \"--ram_cache\", ram_cache_env,\n",
    "            \"--model_name\", \"vit_base_patch16_224\",\n",
    "            \"--batch_size\", batch_size,\n",
    "            \"--num_epochs\", \"50\",\n",
    "            \"--learning_rate\", \"1e-4\",\n",
    "            \"--save_dir\", str(CHECKPOINTS_DIR),\n",
    "            \"--val_split\", \"0.2\",\n",
    "            \"--use_focal_loss\"  # Use focal loss for imbalanced classes\n",
    "        ]\n",
    "    else:\n",
    "        train_cmd = [\n",
    "            sys.executable,\n",
    "            str(train_lora_path),\n",
    "            \"--json_path\", json_path,\n",
    "            \"--images_dir\", images_dir,\n",
    "            \"--model_name\", \"vit_base_patch16_224\",\n",
    "            \"--batch_size\", batch_size,\n",
    "            \"--num_epochs\", \"50\",\n",
    "            \"--learning_rate\", \"1e-4\",\n",
    "            \"--save_dir\", str(CHECKPOINTS_DIR),\n",
    "            \"--val_split\", \"0.2\",\n",
    "            \"--use_focal_loss\"  # Use focal loss for imbalanced classes\n",
    "        ]\n",
    "    \n",
    "    print(f\"\\nRunning LoRA training: {' '.join(train_cmd)}\")\n",
    "    result = subprocess.run(train_cmd, cwd=str(NEW_DIR))\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nâœ“ LoRA training completed successfully!\")\n",
    "        print(f\"âœ“ LoRA adapters saved to: {CHECKPOINTS_DIR / 'lora_adapters'}\")\n",
    "        print(f\"âœ“ Best model saved to: {CHECKPOINTS_DIR / 'best_model_lora.pth'}\")\n",
    "    else:\n",
    "        print(f\"\\nâœ— LoRA training failed with exit code {result.returncode}\")\n",
    "else:\n",
    "    print(\"âœ— Could not find dataset files. Please check the dataset paths.\")\n",
    "    print(f\"\\nSearched in: {DATASETS_DIR}\")\n",
    "    print(\"\\nAvailable directories:\")\n",
    "    for d in DATASETS_DIR.iterdir():\n",
    "        if d.is_dir():\n",
    "            print(f\"  - {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths (update these based on your setup)\n",
    "checkpoint_path = CHECKPOINTS_DIR / \"best_model.pth\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INFERENCE SETUP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Checkpoint path: {checkpoint_path}\")\n",
    "print(f\"JSON path: {json_path if json_path else 'Not found'}\")\n",
    "print(f\"Images directory: {images_dir if images_dir else 'Not found'}\")\n",
    "\n",
    "if json_path and images_dir and checkpoint_path.exists():\n",
    "    print(\"\\nâœ“ All required files found!\")\n",
    "    print(\"\\nTo run inference:\")\n",
    "    print(\"1. Upload an image file to the current directory\")\n",
    "    print(f\"2. Update the image_path variable below\")\n",
    "    print(\"3. Run the inference command\")\n",
    "    \n",
    "    # Example: Find uploaded images in current directory\n",
    "    uploaded_images = list(Path.cwd().glob(\"*.png\")) + list(Path.cwd().glob(\"*.jpg\")) + list(Path.cwd().glob(\"*.jpeg\"))\n",
    "    \n",
    "    if uploaded_images:\n",
    "        image_path = str(uploaded_images[0])\n",
    "        print(f\"\\nâœ“ Found uploaded image: {image_path}\")\n",
    "        print(\"\\nRunning inference...\")\n",
    "        \n",
    "        import subprocess\n",
    "        inference_cmd = [\n",
    "            sys.executable,\n",
    "            str(NEW_DIR / \"inference.py\"),\n",
    "            \"--checkpoint\", str(checkpoint_path),\n",
    "            \"--json_path\", json_path,\n",
    "            \"--images_dir\", images_dir,\n",
    "            \"--image\", image_path,\n",
    "            \"--top_k\", \"5\"\n",
    "        ]\n",
    "        \n",
    "        result = subprocess.run(inference_cmd, cwd=str(NEW_DIR))\n",
    "        if result.returncode == 0:\n",
    "            print(\"\\nâœ“ Inference completed successfully!\")\n",
    "        else:\n",
    "            print(f\"\\nâœ— Inference failed with exit code {result.returncode}\")\n",
    "    else:\n",
    "        print(\"\\nâš  No image files found in current directory\")\n",
    "        print(\"Please upload an image file (.png, .jpg, .jpeg) to:\", Path.cwd())\n",
    "        print(\"\\nExample inference command (after uploading image):\")\n",
    "        print(f\"python {NEW_DIR / 'inference.py'} \\\\\")\n",
    "        print(f\"    --checkpoint {checkpoint_path} \\\\\")\n",
    "        print(f\"    --json_path {json_path} \\\\\")\n",
    "        print(f\"    --images_dir {images_dir} \\\\\")\n",
    "        print(f\"    --image <your_image_file> \\\\\")\n",
    "        print(f\"    --top_k 5\")\n",
    "else:\n",
    "    missing = []\n",
    "    if not json_path:\n",
    "        missing.append(\"JSON file\")\n",
    "    if not images_dir:\n",
    "        missing.append(\"Images directory\")\n",
    "    if not checkpoint_path.exists():\n",
    "        missing.append(\"Checkpoint file\")\n",
    "    print(f\"\\nâœ— Missing required files: {', '.join(missing)}\")\n",
    "    print(\"Please train the model first or check paths.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = CHECKPOINTS_DIR / \"best_model.pth\"\n",
    "\n",
    "if json_path and images_dir and checkpoint_path.exists():\n",
    "    print(\"=\"*60)\n",
    "    print(\"EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Checkpoint: {checkpoint_path}\")\n",
    "    print(f\"JSON: {json_path}\")\n",
    "    print(f\"Images: {images_dir}\")\n",
    "    print(\"\\nRunning evaluation...\")\n",
    "    \n",
    "    import subprocess\n",
    "    eval_cmd = [\n",
    "        sys.executable,\n",
    "        str(NEW_DIR / \"evaluate.py\"),\n",
    "        \"--checkpoint\", str(checkpoint_path),\n",
    "        \"--json_path\", json_path,\n",
    "        \"--images_dir\", images_dir,\n",
    "        \"--output\", str(WORK_DIR / \"evaluation_results.json\")\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(eval_cmd, cwd=str(NEW_DIR))\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"\\nâœ“ Evaluation completed!\")\n",
    "        print(f\"Results saved to: {WORK_DIR / 'evaluation_results.json'}\")\n",
    "    else:\n",
    "        print(f\"\\nâœ— Evaluation failed with exit code {result.returncode}\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    missing = []\n",
    "    if not json_path:\n",
    "        missing.append(\"JSON file\")\n",
    "    if not images_dir:\n",
    "        missing.append(\"Images directory\")\n",
    "    if not checkpoint_path.exists():\n",
    "        missing.append(\"Checkpoint file\")\n",
    "    print(f\"âœ— Missing required files: {', '.join(missing)}\")\n",
    "    print(\"Please train the model first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SAVE RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# List what to save\n",
    "save_items = {\n",
    "    \"Checkpoints\": CHECKPOINTS_DIR,\n",
    "    \"Evaluation Results\": WORK_DIR / \"evaluation_results.json\",\n",
    "    \"Training History\": CHECKPOINTS_DIR / \"training_history.json\",\n",
    "}\n",
    "\n",
    "print(\"\\nAvailable items to save:\")\n",
    "for name, path in save_items.items():\n",
    "    if path.exists():\n",
    "        if path.is_dir():\n",
    "            size = sum(f.stat().st_size for f in path.rglob('*') if f.is_file()) / 1e6\n",
    "            print(f\"  âœ“ {name}: {path} ({size:.2f} MB)\")\n",
    "        else:\n",
    "            size = path.stat().st_size / 1e6\n",
    "            print(f\"  âœ“ {name}: {path} ({size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"  âœ— {name}: {path} (not found)\")\n",
    "\n",
    "# Save to user home or specified location\n",
    "save_location = Path.home() / \"hieroglyph_classifier_results\"\n",
    "save_location.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nSaving to: {save_location}\")\n",
    "\n",
    "for name, source in save_items.items():\n",
    "    if source.exists():\n",
    "        if source.is_dir():\n",
    "            dest = save_location / source.name\n",
    "            if dest.exists():\n",
    "                shutil.rmtree(dest)\n",
    "            shutil.copytree(source, dest)\n",
    "            print(f\"  âœ“ Saved {name} to {dest}\")\n",
    "        else:\n",
    "            dest = save_location / source.name\n",
    "            shutil.copy2(source, dest)\n",
    "            print(f\"  âœ“ Saved {name} to {dest}\")\n",
    "\n",
    "print(f\"\\nâœ“ All results saved to: {save_location}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP-BY-STEP INSTRUCTIONS\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"STEP-BY-STEP INSTRUCTIONS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\")\n",
    "print(\"CELL 1: Install Dependencies\")\n",
    "print(\"  - Checks and installs required packages\")\n",
    "print(\"  - Detects GPU (RTX 8000) and optimizes settings\")\n",
    "print(\"  - Shows system info (RAM, disk space)\")\n",
    "print(\"\")\n",
    "print(\"CELL 2: Set Up Directories\")\n",
    "print(\"  - Finds writable directory (scratch/tmp preferred)\")\n",
    "print(\"  - Creates working directories\")\n",
    "print(\"  - Sets up Python paths\")\n",
    "print(\"\")\n",
    "print(\"CELL 3: Clone Repository OR Upload Files\")\n",
    "print(\"  Option A: Clones from GitHub (recommended)\")\n",
    "print(\"  Option B: Processes uploaded files from current directory\")\n",
    "print(\"\")\n",
    "print(\"CELL 4: Download Datasets\")\n",
    "print(\"  - Automatically filters datasets based on available disk space\")\n",
    "print(\"  - Always downloads essential dataset (Homogenized)\")\n",
    "print(\"  - Downloads optional datasets only if space allows\")\n",
    "print(\"  - Skips datasets that already exist\")\n",
    "print(\"\")\n",
    "print(\"CELL 5: Preload Dataset into RAM\")\n",
    "print(\"  - Loads all training images into RAM (uses 64GB RAM)\")\n",
    "print(\"  - Creates cache file for future use\")\n",
    "print(\"  - Eliminates disk I/O during training\")\n",
    "print(\"  - Set USE_RAM_CACHE = True to enable\")\n",
    "print(\"\")\n",
    "print(\"CELL 6: Verify Setup\")\n",
    "print(\"  - Checks that all required files are present\")\n",
    "print(\"  - Verifies dataset paths\")\n",
    "print(\"  - Confirms GPU availability\")\n",
    "print(\"\")\n",
    "print(\"CELL 7: Run Training\")\n",
    "print(\"  - Uses RAM cache if available (much faster)\")\n",
    "print(\"  - Automatically uses batch size 64 for RTX 8000\")\n",
    "print(\"  - Saves checkpoints during training\")\n",
    "print(\"\")\n",
    "print(\"CELL 8: Run Inference (Optional)\")\n",
    "print(\"  - Classifies hieroglyph images\")\n",
    "print(\"  - Upload image to current directory first\")\n",
    "print(\"\")\n",
    "print(\"CELL 9: Evaluate Model (Optional)\")\n",
    "print(\"  - Evaluates model on test set\")\n",
    "print(\"  - Generates evaluation metrics\")\n",
    "print(\"\")\n",
    "print(\"CELL 10: Save Results (Optional)\")\n",
    "print(\"  - Copies checkpoints and results to persistent location\")\n",
    "print(\"  - Saves to $HOME/hieroglyph_classifier_results\")\n",
    "print(\"\")\n",
    "print(\"CELL 11: Cleanup (Optional)\")\n",
    "print(\"  - Removes old checkpoints\")\n",
    "print(\"  - Cleans temporary files\")\n",
    "print(\"  - Frees disk space\")\n",
    "print(\"\")\n",
    "print(\"=\"*80)\n",
    "print(\"QUICK START:\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. Run cells 1-4 sequentially (setup)\")\n",
    "print(\"2. Run cell 5 to preload data into RAM\")\n",
    "print(\"3. Run cell 6 to verify everything is ready\")\n",
    "print(\"4. Run cell 7 to start training\")\n",
    "print(\"5. Training will use RAM cache automatically (faster)\")\n",
    "print(\"6. Use cells 8-10 for inference/evaluation/saving\")\n",
    "print(\"\")\n",
    "print(\"MEMORY OPTIMIZATION:\")\n",
    "print(\"  - 64GB RAM: All training data loaded into RAM\")\n",
    "print(\"  - 48GB VRAM (RTX 8000): Large batch sizes (64)\")\n",
    "print(\"  - No disk I/O during training = much faster\")\n",
    "print(\"\")\n",
    "print(\"TROUBLESHOOTING:\")\n",
    "print(\"  - Low disk space: Notebook auto-filters datasets\")\n",
    "print(\"  - RAM issues: Set USE_RAM_CACHE = False in cell 5\")\n",
    "print(\"  - GPU not detected: Check CUDA installation\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
